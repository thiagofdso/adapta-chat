# Project Architecture

This document provides an overview of the project's architecture, components, and file structure.

## 1. High-Level Overview

This project is a Python application designed for orchestrating conversations and debates with multiple Large Language Models (LLMs) through the Adapta.one API. It features a modular architecture that separates the API client, content generation logic, and user interfaces.

The user interacts with the system via web interfaces built with Streamlit. These interfaces use a set of "generators" to process user input and get responses from different AI models.

## 2. Core Components

### 2.1. Configuration (`src/config.py`)
- **Purpose:** Manages application settings.
- **Details:** Uses `pydantic-settings` to load sensitive information (like API cookies and session IDs) from a `.env` file, keeping credentials separate from the code.

### 2.2. API Client (`src/generators/adapta/client.py`)
- **Purpose:** Handles all communication with the Adapta.one API.
- **Details:** An asynchronous client built on `httpx`. It manages authentication, session tokens, and provides core methods for calling the AI models. It is designed to be resilient, handling event loop issues when used with Streamlit.

### 2.3. Generator Abstraction (`src/generators/`)
- **Purpose:** To provide a consistent interface for different AI models.
- **`base.py`:** Defines the `BaseContentGenerator` abstract class. This class enforces a contract that all specific generator implementations must follow (e.g., must have a `call_model_with_messages` method).
- **`*_generator.py` files:** These are concrete implementations (`GeminiGenerator`, `ClaudeGenerator`, `GPTGenerator`). They inherit from `BaseContentGenerator` and use the `AdaptaClient` to perform their tasks. This design makes it easy to add new AI models in the future.

### 2.4. User Interfaces (`src/app_*.py`)
- **Purpose:** To provide interactive web interfaces for the user.
- **Technology:** Built with Streamlit.
- **`app_chat.py`:** A simple, single-thread chat application for direct conversation with a chosen AI model.
- **`app_debate.py`:** A complex, multi-agent simulation application. It orchestrates a debate between several AI agents to collaboratively solve a problem, running the agents in parallel for each round of debate.

## 3. Project File Structure

Here is a breakdown of the key files and directories in the project:

```
.
├── docs/
│   └── architecture.md       # This document.
├── logs/
│   └── adapta-chat.log       # Log file generated by the application.
├── src/
│   ├── __init__.py           # Makes 'src' a Python package.
│   ├── app_chat.py           # Streamlit UI for the simple chat.
│   ├── app_debate.py         # Streamlit UI for the multi-agent debate.
│   ├── config.py             # Application configuration and .env loader.
│   ├── generators/
│   │   ├── __init__.py
│   │   ├── base.py           # Abstract base class for all generators.
│   │   └── adapta/
│   │       ├── __init__.py
│   │       ├── client.py     # The Adapta.one API client.
│   │       ├── claude_generator.py
│   │       ├── gemini_generator.py
│   │       └── gpt_generator.py
│   ├── prompts/              # Stores text files with prompts for the AI.
│   └── utils/
│       ├── __init__.py
│       ├── logger.py         # Logging configuration using Loguru.
│       └── text_cleaner.py   # Utility functions, e.g., for cleaning AI responses.
├── .env.example              # Example environment file.
├── .gitignore                # Specifies files for Git to ignore.
├── GEMINI.md                 # Development guidelines for the Gemini agent.
├── pyproject.toml            # Project definition and dependencies for Poetry.
├── README.md                 # General project information and setup instructions.
└── test_adapta_generators.py # Test script for the generators.
```
